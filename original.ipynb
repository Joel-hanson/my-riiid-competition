{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from cairosvg import svg2png\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "\n",
    "KAGGLE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RIIID env init\n",
    "import riiideducation\n",
    "\n",
    "env = riiideducation.make_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV_PATH = 'train.feather'\n",
    "QUESTIONS_CSV_PATH = 'questions.csv'\n",
    "LECTURES_CSV_PATH = 'lectures.csv'\n",
    "SAMPLE_CSV_PATH = 'example_sample_submission.csv'\n",
    "SAMPLE_TEST_CSV_PATH = 'example_test.csv'\n",
    "\n",
    "if KAGGLE:\n",
    "    TRAIN_CSV_PATH = '/kaggle/input/riiid-test-answer-prediction/train.csv'\n",
    "    QUESTIONS_CSV_PATH = '/kaggle/input/riiid-test-answer-prediction/questions.csv'\n",
    "    LECTURES_CSV_PATH = '/kaggle/input/riiid-test-answer-prediction/lectures.csv'\n",
    "    SAMPLE_CSV_PATH = '/kaggle/input/riiid-test-answer-prediction/example_sample_submission.csv'\n",
    "\n",
    "COLUMN_TYPES = {\n",
    "    'row_id': 'int64',\n",
    "    'timestamp': 'int64',\n",
    "    'user_id': 'int32',\n",
    "    'content_id': 'int16',\n",
    "    'content_type_id': 'int8',\n",
    "    'task_container_id': 'int16',\n",
    "    'user_answer': 'int8',\n",
    "    'answered_correctly': 'int8',\n",
    "    'prior_question_elapsed_time': 'float32',\n",
    "    'prior_question_had_explanation': 'boolean'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(TRAIN_CSV_PATH, nrows=10**5, dtype=COLUMN_TYPES)\n",
    "train_df = pd.read_feather(TRAIN_CSV_PATH)\n",
    "train_df = train_df.iloc[:1 * (10 ** 5)]\n",
    "questions_df = pd.read_csv(QUESTIONS_CSV_PATH)\n",
    "lectures_df = pd.read_csv(LECTURES_CSV_PATH)\n",
    "sample_test_df = pd.read_csv(SAMPLE_TEST_CSV_PATH)"
   ]
  },
  {
   "source": [
    "### Common Variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The question based information dict\n",
    "# NOTE: generated only after we move to the training data\n",
    "individual_question_min_time_dict = {}\n",
    "individual_question_mean_time_dict = {}\n",
    "average_question_timestamp_difference_dict = {}\n",
    "bundle_time_relation_dict = {}\n",
    "\n",
    "# check average user answering time\n",
    "user_average_time_to_elapsed_dict = train_df.groupby(\"user_id\").prior_question_elapsed_time.mean().to_dict()\n",
    "# check average question answering time\n",
    "average_question_prior_question_elapsed_time_dict = train_df.groupby([\"content_id\"]).prior_question_elapsed_time.mean().to_dict()\n",
    "# The correct answered questions\n",
    "check_answered_correctly = (train_df.answered_correctly == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for question specific features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_df[questions_df.tags.isna()] = questions_df[questions_df.tags.isna()].fillna(\"\")\n",
    "questions_df[questions_df.tags.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_counts = questions_df[['tags', 'question_id']].groupby('tags')['tags'].count()\n",
    "tag_counts_list = list(tag_counts.sort_values().index.values)\n",
    "questions_df['tag_count_wise_id'] = questions_df.apply(lambda row: tag_counts_list.index(row.tags), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the wrong and right question tags based on correctly answered\n",
    "tags_list = [value.split() for value in questions_df.tags.values]\n",
    "\n",
    "questions_df['tags_list'] = tags_list\n",
    "questions_df['number_of_tags'] = questions_df.apply(lambda row: len(row.tags), axis=1)\n",
    "questions_df['tags'] = questions_df['tags'].astype(str)\n",
    "\n",
    "questions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The part content type of the question wheather if it is a listening section or a reading secition\n",
    "questions_df['part_test_listening'] = True\n",
    "reading_sections_parts = [5, 6, 7]\n",
    "questions_df.loc[questions_df.part.isin(reading_sections_parts), 'part_test_listening'] = False\n",
    "questions_df.part_test_listening.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the complete tags of the question\n",
    "tags = [value.split() for value in questions_df[questions_df.tags != \"nan\"].tags.values]\n",
    "tags = [item for elem in tags for item in elem]\n",
    "tags = set(tags)\n",
    "tags = list(tags)\n",
    "print(f'There are {len(tags)} different tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the wrong and right question based on correctly answered\n",
    "correct = train_df.groupby([\"content_id\", 'answered_correctly'], as_index=False).size()\n",
    "correct = correct.pivot(index= \"content_id\", columns='answered_correctly', values='size')\n",
    "correct.columns = ['wrong', 'right']\n",
    "correct = correct.fillna(0)\n",
    "correct[['wrong', 'right']] = correct[['wrong', 'right']].astype(int)\n",
    "questions_df = questions_df.merge(correct, left_on = \"question_id\", right_on = \"content_id\", how = \"left\")\n",
    "questions_df['percentage_correct'] = questions_df.right / (questions_df.right + questions_df.wrong)\n",
    "questions_df.head()\n",
    "\n",
    "# questions_df.drop([\"wrong_x\", \"right_x\", \"wrong_y\", \"right_y\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This seems to be wrong please check again\n",
    "train_df['question_bundle_id'] = train_df.apply(lambda row: question_dict[row.content_id].get('bundle_id'), axis=1)\n",
    "correct = train_df.groupby([\"question_bundle_id\", 'answered_correctly'], as_index=False).size()\n",
    "correct = correct.pivot(index= \"question_bundle_id\", columns='answered_correctly', values='size')\n",
    "correct.columns = ['bundle_wrong', 'bundle_right']\n",
    "correct = correct.fillna(0)\n",
    "correct[['bundle_wrong', 'bundle_right']] = correct[['bundle_wrong', 'bundle_right']].astype(int)\n",
    "\n",
    "questions_df = questions_df.merge(correct, left_on = \"bundle_id\", right_on = \"question_bundle_id\", how = \"left\")\n",
    "questions_df['task_percentage_correct'] = questions_df.bundle_right / (questions_df.bundle_right + questions_df.bundle_wrong)\n",
    "questions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_df = pd.DataFrame()\n",
    "for x in range(len(tags)):\n",
    "    df = questions_df[questions_df.tags.apply(lambda l: tags[x] in l)]\n",
    "    df1 = df.agg({'wrong': ['sum'], 'right': ['sum']})\n",
    "    df1['total_questions'] = df1.wrong + df1.right\n",
    "    df1['question_ids_with_tag'] = len(df)\n",
    "    df1['tag'] = tags[x]\n",
    "    df1 = df1.set_index('tag')\n",
    "    tags_df = tags_df.append(df1)\n",
    "\n",
    "tags_df[['wrong', 'right', 'total_questions']] = tags_df[['wrong', 'right', 'total_questions']].astype(int)\n",
    "tags_df['percent_correct'] = tags_df.right / tags_df.total_questions\n",
    "tags_df = tags_df.sort_values(by = \"percent_correct\")\n",
    "\n",
    "tags_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question dict\n",
    "question_dict = questions_df[['question_id', 'percentage_correct', 'tags', 'part', 'bundle_id', 'tags_list']].set_index('question_id').to_dict()"
   ]
  },
  {
   "source": [
    "### The lecture specific data creation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The lecture based dict for type of and part\n",
    "lecture_dict = lectures_df.set_index(\"lecture_id\").to_dict(orient=\"index\")\n",
    "lecture_comprehensive_type_of_dict = lectures_df.set_index('type_of').groupby(level=0).apply(lambda row: row.to_dict('list')).to_dict()\n",
    "lecture_comprehensive_part_dict = lectures_df.set_index('part').groupby(level=0).apply(lambda row: row.to_dict('list')).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in lecture_comprehensive_part_dict:\n",
    "    lecture_comprehensive_part_dict[key]['tag'] = list(set(lecture_comprehensive_part_dict[key]['tag']))\n",
    "    lecture_comprehensive_part_dict[key]['type_of'] = list(set(lecture_comprehensive_part_dict[key]['type_of']))\n",
    "\n",
    "for key in lecture_comprehensive_type_of_dict:\n",
    "    lecture_comprehensive_type_of_dict[key]['tag'] = list(set(lecture_comprehensive_type_of_dict[key]['tag']))\n",
    "    lecture_comprehensive_type_of_dict[key]['part'] = list(set(lecture_comprehensive_type_of_dict[key]['part']))"
   ]
  },
  {
   "source": [
    "### Common function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_timestamp_in_parts(millis):\n",
    "    seconds = (millis / 1000)\n",
    "    minutes = (seconds / 60)\n",
    "    hours = (minutes / 60)\n",
    "    days = (hours / 24)\n",
    "    return days, hours, minutes, seconds\n",
    "\n",
    "def get_timestamp_in_parts(row):\n",
    "    millis = row.timestamp\n",
    "    seconds = (millis / 1000)\n",
    "    minutes = (seconds / 60)\n",
    "    hours = (minutes / 60)\n",
    "    days = (hours / 24)\n",
    "    return days, hours\n",
    "\n",
    "def get_prior_elasped_time_in_parts(row):\n",
    "    prior_millis = row.prior_question_elapsed_time\n",
    "    prior_seconds = (prior_millis / 1000)\n",
    "    prior_minutes = (prior_seconds / 60)\n",
    "    return prior_minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retention\n",
    "# https://en.wikipedia.org/wiki/Forgetting_curve\n",
    "# https://psychology.stackexchange.com/questions/5199/which-equation-is-ebbinghauss-forgetting-curve-and-what-do-the-constants-repres\n",
    "def apply_filter_based_on_previous_row(func):\n",
    "    prev_row = {\n",
    "        \"has_seen_lecture\": False,\n",
    "        \"previous_lecture_hours\": 0,\n",
    "        \"previous_lecture\": {},\n",
    "        \"user_id\": \"\"\n",
    "    }\n",
    "    def wrapper(curr_row, **kwargs):\n",
    "        content_id = curr_row['content_id']\n",
    "        tag_of_prev_lecture = 0\n",
    "        part_of_prev_lecture = 0\n",
    "        has_seen_same_tag_as_lecture = False\n",
    "        has_seen_same_part_as_lecture = False\n",
    "        has_part_common_with_type_of = False\n",
    "        has_tag_common_with_type_of = False\n",
    "        has_tag_common_with_part_dict = False\n",
    "        has_type_of_common_with_part_dict = False\n",
    "        if prev_row['user_id'] == curr_row['user_id']:\n",
    "            if curr_row['content_type_id'] != 0:\n",
    "                prev_row['has_seen_lecture'] = True\n",
    "                prev_row['previous_lecture_hours'] = curr_row['event_in_hours']\n",
    "                prev_row['previous_lecture'] = lecture_dict[content_id]\n",
    "            else:\n",
    "                has_seen_same_tag_as_lecture = (str(prev_row['previous_lecture'].get(\"tag\", \"\")) in str(question_dict[content_id]['tags']).split())\n",
    "                has_seen_same_part_as_lecture = (prev_row['previous_lecture'].get(\"part\") == question_dict[content_id]['part'])\n",
    "                prev_type_of = prev_row['previous_lecture'].get(\"type_of\")\n",
    "                prev_part = prev_row['previous_lecture'].get(\"part\")\n",
    "                has_part_common_with_type_of = question_dict[content_id]['part'] in  lecture_comprehensive_type_of_dict.get(prev_type_of, {}).get('part', [])\n",
    "                has_tag_common_with_type_of = bool(set(map(int, str(question_dict[content_id]['tags']).split())).intersection(lecture_comprehensive_type_of_dict.get(prev_type_of, {}).get('tag', [])))\n",
    "                has_tag_common_with_part_dict = bool(set(map(int, str(question_dict[content_id]['tags']).split())).intersection(lecture_comprehensive_part_dict.get(prev_part, {}).get('tag', [])))\n",
    "                has_type_of_common_with_part_dict = prev_type_of in lecture_comprehensive_part_dict[question_dict[content_id]['part']]['type_of']\n",
    "                tag_of_prev_lecture = prev_row['previous_lecture'].get('tag', 0)\n",
    "                part_of_prev_lecture = prev_row['previous_lecture'].get('part', 0)\n",
    "        else:\n",
    "            prev_row['user_id'] = curr_row['user_id']\n",
    "            if curr_row['content_type_id'] != 0:\n",
    "                prev_row['has_seen_lecture'] = True\n",
    "                prev_row['previous_lecture_hours'] = curr_row['event_in_hours']\n",
    "                prev_row['previous_lecture'] = lecture_dict[content_id]\n",
    "            else:\n",
    "                prev_row['has_seen_lecture'] = False\n",
    "                prev_row['previous_lecture_hours'] = 0\n",
    "                prev_row['previous_lecture'] = {}\n",
    "\n",
    "        timestamp_difference = curr_row['event_in_hours'] - prev_row['previous_lecture_hours']\n",
    "        retention = func(timestamp_difference)\n",
    "        return retention, tag_of_prev_lecture, part_of_prev_lecture, prev_row['has_seen_lecture'], has_seen_same_tag_as_lecture, has_seen_same_part_as_lecture, has_part_common_with_type_of, has_tag_common_with_type_of, has_tag_common_with_part_dict, has_type_of_common_with_part_dict\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "@apply_filter_based_on_previous_row\n",
    "def running_retention(timestamp_difference):\n",
    "    retention = 1.48 / ((1.25 * timestamp_difference) + 1.48)\n",
    "    return retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lag time\n",
    "def get_shift_values(df):\n",
    "    event_time = df[['user_id', 'timestamp']].groupby('user_id')['timestamp'].diff()\n",
    "    shift_event_time = df[['event_time', 'user_id']].groupby('user_id').event_time.shift(-1)\n",
    "    shift_elapsed_time = df[['prior_question_elapsed_time', 'user_id']].groupby('user_id').prior_question_elapsed_time.shift(-1)\n",
    "    shift_prior_question_had_explanation = df[['user_id', 'prior_question_had_explanation']].groupby('user_id').prior_question_had_explanation.shift(-1)\n",
    "\n",
    "    event_lag_time = df['shift_event_time'] - df['shift_elapsed_time']\n",
    "    return event_time, shift_event_time, shift_elapsed_time, shift_prior_question_had_explanation, event_lag_time\n",
    "\n",
    "def get_prior_elapsed_time_difference(content_id, shift_elapsed_time):\n",
    "    question_min_time = individual_question_min_time_dict[content_id]\n",
    "    return shift_elapsed_time - question_min_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check based on the average time taken for question\n",
    "def has_elapsed_time_greater_than_average_time(content_id, shift_elapsed_time):\n",
    "    question_min_time = individual_question_mean_time_dict[content_id]\n",
    "    return shift_elapsed_time > question_min_time\n",
    "\n",
    "def has_event_time_greater_than_average(content_id, event_time):\n",
    "    average_question_time = average_question_timestamp_difference_dict.get(content_id, 0)\n",
    "    return event_time > average_question_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question time list\n",
    "bundle_time_relation_dict = {}\n",
    "prev_row = {\n",
    "    \"user_id\": \"\",\n",
    "    \"bundle_id\": \"\",\n",
    "    \"previous_bundle_elapsed_time\": \"\"\n",
    "}\n",
    "\n",
    "\n",
    "def get_question_time_taken(curr_row):\n",
    "    current_bundle_id = question_dict[curr_row.content_id]['bundle_id']\n",
    "    if prev_row['user_id'] == curr_row['user_id']:\n",
    "        if prev_row['bundle_id'] == current_bundle_id:\n",
    "            time_taken_for_question = prev_row['previous_bundle_elapsed_time']\n",
    "        else:\n",
    "            time_taken_for_question = curr_row['prior_question_elapsed_time']\n",
    "            prev_row['previous_bundle_elapsed_time'] = curr_row['prior_question_elapsed_time']\n",
    "            prev_row['bundle_id'] = current_bundle_id\n",
    "    else:\n",
    "        prev_row['bundle_id'] = current_bundle_id\n",
    "        prev_row['previous_bundle_elapsed_time'] = curr_row['prior_question_elapsed_time']\n",
    "        time_taken_for_question = 0\n",
    "    prev_row['user_id'] = curr_row.user_id\n",
    "    return time_taken_for_question\n",
    "\n",
    "\n",
    "for index, row in train_df.iloc[::-1].iterrows():\n",
    "    if row.content_id in bundle_time_relation_dict and bundle_time_relation_dict:\n",
    "        bundle_time_relation_dict[row.content_id]['question_time_list'].append(get_question_time_taken(row))\n",
    "    else:\n",
    "        bundle_time_relation_dict[row.content_id] = {\n",
    "            \"question_time_list\": [get_question_time_taken(row)]\n",
    "        }\n",
    "\n",
    "for content_id in bundle_time_relation_dict.keys():\n",
    "    bundle_time_relation_dict[content_id]['minimum_time'] = min(bundle_time_relation_dict[content_id]['question_time_list'])\n",
    "    bundle_time_relation_dict[content_id]['average_time'] = np.mean(bundle_time_relation_dict[content_id]['question_time_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bundle_lag_time(row):\n",
    "    if row.shift_elapsed_time > bundle_time_relation_dict[row.content_id]['minimum_time']:\n",
    "        return row.shift_elapsed_time - bundle_time_relation_dict[row.content_id]['minimum_time']\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: please check the values that is used in each question they might be wrong\n",
    "def is_question_above_average_answering(row):\n",
    "    percentage_correct = question_dict[row.content_id].get('percentage_correct')\n",
    "    if percentage_correct:\n",
    "        return percentage_correct > 50\n",
    "    return False\n",
    "\n",
    "def is_all_tags_above_average_answering(row):\n",
    "    tags_answer_percentages = []\n",
    "    tags = question_dict[row.content_id].get('tags_list')\n",
    "    if not tags:\n",
    "        return False\n",
    "    tags = tags\n",
    "    for tag in tags:\n",
    "        tags_answer_percentages.append(tags_df.loc[tag].percent_correct)\n",
    "    overall_tag_percentage = sum(tags_answer_percentages) / len(tags)\n",
    "    return overall_tag_percentage > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum_std_correctness(df, group_by_feature, target):\n",
    "    feature_agg = df[[group_by_feature, target]].groupby(group_by_feature)[target].agg(['sum', 'count','var'])\n",
    "    feature_agg = feature_agg.astype('float32')\n",
    "\n",
    "    feature_sum = df['bundle_id'].map(feature_agg['sum']).astype('int32').fillna(0)\n",
    "    feature_std = df['bundle_id'].map(feature_agg['var']).astype('float16').fillna(0)\n",
    "    feature_correctness = df['bundle_id'].map(feature_agg['sum'] / feature_agg['count'])\n",
    "\n",
    "    feature_correctness = feature_correctness.astype('float16')\n",
    "    return feature_sum, feature_std, feature_correctness\n",
    "\n",
    "def get_cum_sum_std_correctness(df, group_by_feature, target):\n",
    "    cum = train_df[[group_by_feature, target]].groupby(group_by_feature)[target].agg(['cumsum', 'cumcount'])\n",
    "    feature_cum_correctness = cum['cumsum'] / cum['cumcount']\n",
    "    feature_correct_cumsum = cum['cumsum'].fillna(0)\n",
    "    feature_correct_cumcount = cum['cumcount'].fillna(0)\n",
    "\n",
    "    feature_cum_correctness = feature_cum_correctness.astype('float16')\n",
    "    feature_correct_cumcount = feature_correct_cumcount.astype('int16')\n",
    "    feature_correct_cumsum = feature_correct_cumsum.astype('int16')\n",
    "    return feature_cum_correctness, feature_correct_cumcount, feature_correct_cumsum"
   ]
  },
  {
   "source": [
    "### Prepare first group test set with train_df"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set User specific features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sort_values(['user_id', 'timestamp'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Get timestamp as hours and minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['event_in_days', 'event_in_hours']] = [*train_df.apply(lambda row: get_timestamp_in_parts(row), axis=1)]\n",
    "train_df['prior_minutes'] = train_df.apply(lambda row: get_prior_elasped_time_in_parts(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Find the intro rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: The intro section can also be calculated based on the bundle id\n",
    "train_df['intro_section'] = train_df.apply(lambda row: ((train_df.timestamp == 0) or (row.task_container_id == 0)), axis=1)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. The actions after the lectures should be tagged with something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"retention\"], train_df[\"previous_lecture_tag\"], train_df[\"previous_lecture_part\"], train_df[\"has_seen_lecture_before\"], train_df[\"has_seen_same_tag_as_lecture\"], train_df[\"has_seen_same_part_as_lecture\"], train_df[\"has_part_common_with_type_of\"], train_df[\"has_tag_common_with_type_of\"], train_df[\"has_tag_common_with_part_dict\"], train_df[\"has_type_of_common_with_part_dict\"] = zip(*train_df.apply(running_retention, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train df only question and not lectures\n",
    "train_df = train_df[train_df['content_type_id'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Now lets find out the lag time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['event_time', 'shift_event_time', 'shift_elapsed_time', 'has_seen_question_explanation', 'event_lag_time']] = get_shift_values(train_df)\n",
    "train_df_elapsed_time_groupby = train_df[['content_id', 'shift_elapsed_time']].groupby(['content_id']).shift_elapsed_time\n",
    "individual_question_min_time_dict = train_df_elapsed_time_groupby.min().to_dict()\n",
    "individual_question_mean_time_dict = train_df_elapsed_time_groupby.mean().to_dict()\n",
    "\n",
    "average_question_timestamp_difference_dict = train_df[(train_df['shift_event_time'] < 3600000)][['content_id', 'shift_event_time']].groupby('content_id').shift_event_time.mean().to_dict()\n",
    "\n",
    "del train_df_elapsed_time_groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['lag_time'] = train_df.apply(lambda row: get_prior_elapsed_time_difference(row.content_id, row.shift_elapsed_time), axis=1)\r\n",
    "train_df['bundle_lag_time'] = train_df.apply(lambda row: get_bundle_lag_time(row), axis=1)\r\n",
    "\r\n",
    "train_df['question_answered_late'] = train_df.apply(lambda row: has_elapsed_time_greater_than_average_time(row.content_id, row.shift_elapsed_time), axis=1)\r\n",
    "train_df['event_time_greater_than_average'] = train_df.apply(lambda row: has_event_time_greater_than_average(row.content_id, row.shift_event_time), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. lets find the average time took for each question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The average time per question can be considered based on the every candidate or based on the current candidate previous question answering time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['question_took_more_than_average_user_time'] = train_df.apply(lambda row: row.prior_question_elapsed_time > user_average_time_to_elapsed_dict[row.user_id], axis=1)\n",
    "train_df['question_took_more_than_average_content_time'] = train_df.apply(lambda row: row.prior_question_elapsed_time > average_question_prior_question_elapsed_time_dict[row.user_id], axis=1)\n",
    "\n",
    "train_df['has_more_than_average_bundle_time'] = train_df.apply(lambda row: row.shift_elapsed_time < bundle_time_relation_dict[row.content_id]['average_time'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Find the toughest questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['question_has_above_average_correctness'] = train_df.apply(lambda row: is_question_above_average_answering(row), axis=1)\n",
    "train_df['tag_has_above_average_correctness'] = train_df.apply(lambda row: is_all_tags_above_average_answering(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. toughtest questions relates to the lag time and the elapsed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_time_answered_correctly_mean_dict = train_df[check_answered_correctly][['content_id', 'lag_time']].groupby(['content_id']).lag_time.mean().to_dict()\n",
    "train_df['has_above_average_lag_time_for_the_question'] = train_df.apply(lambda row: row.lag_time > lag_time_answered_correctly_mean_dict.get(row.content_id, 0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_elapsed_time_answered_correctly_mean_dict = train_df[check_answered_correctly][['content_id', 'shift_elapsed_time']].groupby(['content_id']).shift_elapsed_time.mean().to_dict()\n",
    "train_df['has_above_average_shift_elpased_time_for_the_question'] = train_df.apply(lambda row: row.shift_elapsed_time > shift_elapsed_time_answered_correctly_mean_dict.get(row.content_id, 0), axis=1)"
   ]
  },
  {
   "source": [
    "##### 7. The prior time mean for each user should be averaged to check if the user too more than usual"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Do the following by setting the time rather than setting the flag\n",
    "# BUG: The comparison seems to be wrong \n",
    "elapsed_time_mean_dict = train_df[check_answered_correctly][['shift_elapsed_time', 'user_id']].groupby('user_id').shift_elapsed_time.mean().to_dict()\n",
    "train_df['has_above_user_average_time_to_answer'] = train_df[['user_id', 'shift_elapsed_time']].apply(lambda row: elapsed_time_mean_dict.get(row.user_id, 0) <= row.shift_elapsed_time, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_time_mean_dict = train_df[check_answered_correctly][['event_time', 'user_id']].groupby('user_id').event_time.mean().to_dict()\n",
    "train_df['has_above_user_average_time_for_event'] = train_df[['user_id', 'event_time']].apply(lambda row: event_time_mean_dict.get(row.user_id, 0) <= row.event_time, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_event_time_mean_dict = train_df[check_answered_correctly][['shift_event_time', 'user_id']].groupby('user_id').shift_event_time.mean().to_dict()\n",
    "train_df['has_above_user_average_time_for_event'] = train_df[['user_id', 'shift_event_time']].apply(lambda row: shift_event_time_mean_dict.get(row.user_id, 0) <= row.shift_event_time, axis=1)"
   ]
  },
  {
   "source": [
    "##### 8. cum correctness of the answers made by the user"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['user_cum_correctness', 'user_correct_cumsum', 'user_correct_cumcount']] = get_cum_sum_std_correctness(train_df, 'user_id', 'answered_correctly')"
   ]
  },
  {
   "source": [
    "##### 9. cum of prior question had seen explanation "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['seen_question_explanation_mean', 'seen_question_explanation_cumsum', 'seen_question_explanation_cumcount']] = get_cum_sum_std_correctness(train_df, 'user_id', 'has_seen_question_explanation')"
   ]
  },
  {
   "source": [
    "##### 10. The attempt feature"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"attempt_no\"] = 1\n",
    "train_df.attempt_no = train_df.attempt_no.astype('int8')\n",
    "train_df[\"attempt_no\"] = train_df[[\"user_id\", \"content_id\", \"attempt_no\"]].groupby([\"user_id\", \"content_id\"])[\"attempt_no\"].cumsum()"
   ]
  },
  {
   "source": [
    "##### 11. The aggregate of the answered correctly with the user_id, content_id"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_columns = ['user_id', 'answered_correctly']\n",
    "train_df[['user_sum', 'user_std', 'user_correctness']] = get_sum_std_correctness(train_df, required_columns)\n",
    "\n",
    "required_columns = ['content_id', 'answered_correctly']\n",
    "train_df[['content_sum', 'content_std', 'content_correctness']] = get_sum_std_correctness(train_df, required_columns)"
   ]
  },
  {
   "source": [
    "##### 12. get harmonic mean of the content"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['hmean_user_content_accuracy'] = 2 * (\n",
    "    (train_df['user_correctness'] * train_df['content_correctness']) /\n",
    "    (train_df['user_correctness'] + train_df['content_correctness'])\n",
    ")"
   ]
  },
  {
   "source": [
    "##### 13. Check if the user clicked the correct answer by chance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['luckily_answered_correctly'] = train_df.apply(lambda row: row.attempt_no == 0 and row.has_seen_question_explanation and row.answered_correctly, axis=1)"
   ]
  },
  {
   "source": [
    "##### 13. The aggregate of the answered correctly with the bundle_id, part, part content type"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.join(questions_df.set_index(\"question_id\"), 'content_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_columns = ['bundle_id', 'answered_correctly']\n",
    "train_df[['bundle_sum', 'bundle_std', 'bundle_correctness']] = get_sum_std_correctness(train_df, required_columns)\n",
    "\n",
    "required_columns = ['part', 'answered_correctly']\n",
    "train_df[['part_sum', 'part_std', 'part_correctness']] = get_sum_std_correctness(train_df, required_columns)\n",
    "\n",
    "required_columns = ['part_test_listening', 'answered_correctly']\n",
    "train_df[['part_test_listening_sum', 'part_test_listening_std', 'part_test_listening_correctness']] = get_sum_std_correctness(train_df, required_columns)"
   ]
  },
  {
   "source": [
    "### Training "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = train_df.groupby('user_id').tail(5)\n",
    "X_train = train_df[~train_df.index.isin(X_val.index)]\n",
    "y_train = X_train.answered_correctly\n",
    "y_val = X_val.answered_correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['user_id', 'content_id',\n",
    "       'task_container_id', 'prior_question_elapsed_time', 'prior_question_had_explanation', 'event_in_days',\n",
    "       'event_in_hours', 'prior_minutes', 'intro_section', 'retention',\n",
    "       'previous_lecture_tag', 'previous_lecture_part',\n",
    "       'has_seen_lecture_before', 'has_seen_same_tag_as_lecture',\n",
    "       'has_seen_same_part_as_lecture', 'has_part_common_with_type_of',\n",
    "       'has_tag_common_with_type_of', 'has_tag_common_with_part_dict',\n",
    "       'has_type_of_common_with_part_dict', 'event_time', 'shift_event_time',\n",
    "       'shift_elapsed_time', 'event_lag_time', 'lag_time',\n",
    "       'question_answered_late', 'event_time_greater_than_average',\n",
    "       'question_took_more_than_average_user_time',\n",
    "       'question_took_more_than_average_content_time',\n",
    "       'has_more_than_average_bundle_time', 'bundle_lag_time',\n",
    "       'question_bundle_id', 'question_has_above_average_correctness',\n",
    "       'tag_has_above_average_correctness',\n",
    "       'has_above_average_lag_time_for_the_question',\n",
    "       'has_above_average_shift_elpased_time_for_the_question',\n",
    "       'has_above_user_average_time_to_answer',\n",
    "       'has_above_user_average_time_for_event', 'user_cum_correctness',\n",
    "       'user_correct_cumsum', 'user_correct_cumcount',\n",
    "       'has_seen_question_explanation', 'seen_question_explanation_mean',\n",
    "       'seen_question_explanation_cumsum', 'seen_question_explanation_cumcount',\n",
    "       'attempt_no', 'content_count', 'content_sum',\n",
    "       'content_correctness', 'user_correctness', 'user_correct_sum',\n",
    "       'user_correct_count', 'hmean_user_content_accuracy', 'bundle_id',\n",
    "       'part', 'tag_count_wise_id',\n",
    "       'number_of_tags', 'wrong', 'right', 'percentage_correct',\n",
    "       'bundle_wrong', 'bundle_right', 'task_percentage_correct', 'bundle_sum',\n",
    "       'bundle_std', 'bundle_correctness', 'part_test_listening', 'part_sum',\n",
    "       'part_std', 'part_correctness', 'part_test_listening_sum',\n",
    "       'part_test_listening_std', 'part_test_listening_correctness', 'luckily_answered_correctly']\n",
    "\n",
    "categorical_feature = ['content_id',\n",
    "       'prior_question_elapsed_time', 'prior_question_had_explanation', 'event_in_days',\n",
    "       'even_in_hours', 'prior_minutes', 'intro_section', 'retention',\n",
    "       'previous_lecture_tag', 'previous_lecture_part',\n",
    "       'has_seen_lecture_before', 'has_seen_same_tag_as_lecture',\n",
    "       'has_seen_same_part_as_lecture', 'has_part_common_with_type_of',\n",
    "       'has_tag_common_with_type_of', 'has_tag_common_with_part_dict',\n",
    "       'has_type_of_common_with_part_dict','question_answered_late', 'event_time_greater_than_average',\n",
    "       'question_took_more_than_average_user_time',\n",
    "       'question_took_more_than_average_content_time',\n",
    "       'has_more_than_average_bundle_time', 'question_has_above_average_correctness',\n",
    "       'tag_has_above_average_correctness',\n",
    "       'has_above_average_lag_time_for_the_question',\n",
    "       'has_above_average_shift_elpased_time_for_the_question',\n",
    "       'has_above_user_average_time_to_answer',\n",
    "       'has_above_user_average_time_for_event', 'has_seen_question_explanation', 'part', 'part_test_listening', 'luckily_answered_correctly']\n",
    "\n",
    "categorical_feature_idxs = []\n",
    "for feature_name in categorical_feature:\n",
    "    try:\n",
    "        categorical_feature_idxs.append(features.index(feature_name))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[features]\n",
    "X_val = X_val[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.has_seen_question_explanation = X_train.has_seen_question_explanation.fillna(False).astype(np.bool)\n",
    "X_val.has_seen_question_explanation = X_val.has_seen_question_explanation.fillna(False).astype(np.bool)\n",
    "\n",
    "X_train.prior_question_had_explanation = X_train.prior_question_had_explanation.fillna(False).astype(np.bool)\n",
    "X_val.prior_question_had_explanation = X_val.prior_question_had_explanation.fillna(False).astype(np.bool)\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {\n",
    "    'objective': 'binary',\n",
    "    \"metric\": 'auc',\n",
    "    'num_leaves': 350,\n",
    "    'max_bin':700,\n",
    "    'min_child_weight': 0.03454472573214212,\n",
    "    'feature_fraction': 0.58,\n",
    "    'bagging_fraction': 0.58,\n",
    "    #'min_data_in_leaf': 106,\n",
    "    'max_depth': -1,\n",
    "    'learning_rate': 0.05,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"bagging_seed\": 11,\n",
    "    \"verbosity\": -1,\n",
    "    'reg_alpha': 0.3899927210061127,\n",
    "    'reg_lambda': 0.6485237330340494,\n",
    "    'random_state': 47\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(\n",
    "    data = X_train,\n",
    "    label = y_train,\n",
    "    categorical_feature = None,\n",
    ")\n",
    "\n",
    "val_data = lgb.Dataset(\n",
    "    data = X_val,\n",
    "    label = y_val,\n",
    "    categorical_feature = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train, y_train, X_val, y_val\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    evals_result = {}\n",
    "    model = lgb.train(\n",
    "        params = lgbm_params,\n",
    "        train_set = train_data,\n",
    "        valid_sets = [val_data],\n",
    "        num_boost_round = 5000,\n",
    "        verbose_eval = 50,\n",
    "        evals_result = evals_result,\n",
    "        early_stopping_rounds = 50, \n",
    "        categorical_feature = categorical_feature_idxs,\n",
    "        feature_name = features,\n",
    "    )\n",
    "\n",
    "    # save model\n",
    "    model.save_model(f'model_v1_2500.lgb')\n",
    "    \n",
    "    return model, evals_result\n",
    "    \n",
    "model, evals_result = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(evals_result):\n",
    "    for metric in ['auc']:\n",
    "        plt.figure(figsize=(20,8))\n",
    "        \n",
    "        for key in evals_result.keys():\n",
    "            history_len = len(evals_result.get(key)[metric])\n",
    "            history = evals_result.get(key)[metric]\n",
    "            x_axis = np.arange(1, history_len + 1)\n",
    "            plt.plot(x_axis, history, label=key)\n",
    "        \n",
    "        x_ticks = list(filter(lambda e: (e % (history_len // 100 * 10) == 0) or e == 1, x_axis))\n",
    "        plt.xticks(x_ticks, fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "\n",
    "        plt.title(f'{metric.upper()} History of training', fontsize=18);\n",
    "        plt.xlabel('EPOCH', fontsize=16)\n",
    "        plt.ylabel(metric.upper(), fontsize=16)\n",
    "        \n",
    "        if metric in ['auc']:\n",
    "            plt.legend(loc='upper left', fontsize=14)\n",
    "        else:\n",
    "            plt.legend(loc='upper right', fontsize=14)\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "plot_history(evals_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the feature importance in terms of gain and split\n",
    "def show_feature_importances(model, importance_type, max_num_features=10**10):\n",
    "    feature_importances = pd.DataFrame()\n",
    "    feature_importances['feature'] = features\n",
    "    feature_importances['value'] = pd.DataFrame(model.feature_importance(importance_type))\n",
    "    feature_importances = feature_importances.sort_values(by='value', ascending=False) # sort feature importance\n",
    "    feature_importances.to_csv(f'feature_importances_{importance_type}.csv') # write feature importance to csv\n",
    "    feature_importances = feature_importances[:max_num_features] # only show max_num_features\n",
    "    \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.xlim([0, feature_importances.value.max()*1.1])\n",
    "    plt.title(f'Feature {importance_type}', fontsize=18);\n",
    "    sns.barplot(data=feature_importances, x='value', y='feature', palette='rocket');\n",
    "    for idx, v in enumerate(feature_importances.value):\n",
    "        plt.text(v, idx, \"  {:.2e}\".format(v))\n",
    "\n",
    "show_feature_importances(model, 'gain')\n",
    "show_feature_importances(model, 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show tree and save as png\n",
    "def save_tree_diagraph(model):\n",
    "    tree_digraph = lgb.create_tree_digraph(model, show_info=['split_gain', 'internal_count'])\n",
    "\n",
    "    tree_png = svg2png(tree_digraph._repr_svg_(), output_width=3840)\n",
    "    tree_png = Image.open(BytesIO(tree_png))\n",
    "\n",
    "    tree_png.save('create_tree_digraph.png')\n",
    "\n",
    "    display(tree_png)\n",
    "    \n",
    "save_tree_diagraph(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove train and validation data to free memory before prediction phase\n",
    "del train_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if KAGGLE:\n",
    "    iter_test = env.iter_test()\n",
    "else:\n",
    "    iter_test = sample_test_df.groupby('group_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_test_df = None\n",
    "\n",
    "for index, (_, sample_prediction_df) in enumerate(iter_test):\n",
    "\n",
    "    if index != 0:\n",
    "        # Do the operation for populating the values for the question answers\n",
    "        previous_test_df['answered_correctly'] = eval(sample_prediction_df['prior_group_answers_correct'].iloc[0])\n",
    "        # previous_test_groups = previous_test_df[['user_id', 'timestamp', 'answered_correctly']].groupby(['user_id', 'timestamp']).groups\n",
    "        previous_group_filter = ((train_df['predict'] == True) & (train_df['answered_correctly'].isna()))\n",
    "        train_df[previous_group_filter]['answered_correctly'] = train_df[previous_group_filter].apply(lambda row: previous_test_df[(previous_test_df.user_id == row.user_id) & (previous_test_df.timestamp == row.timestamp)].values[0], axis=1)\n",
    "\n",
    "    train_df['predict'] = False\n",
    "    previous_test_df = sample_prediction_df.copy()\n",
    "    sample_prediction_df.drop(['prior_group_answers_correct', 'prior_group_responses'])\n",
    "    sample_prediction_df['predict'] = True\n",
    "    sample_prediction_df = sample_prediction_df.join(questions_df.set_index(\"question_id\"), 'content_id', how='left')\n",
    "\n",
    "    train_df = train_df.concat([train_df, sample_prediction_df])\n",
    "    train_df.sort_values(['user_id', 'timestamp'], inplace=True)\n",
    "\n",
    "    predict_filter = (train_df['predict'] == True)\n",
    "    sample_prediction_users = sample_prediction_df['user_id'].unique()\n",
    "    must_predict_users = (train_df['user_id'].isin(sample_prediction_users))\n",
    "    # ### Prepare first group test set with train_df\n",
    "    # ### Set User specific features\n",
    "    # ##### 1. Get timestamp as hours and minutes\n",
    "    train_df[['event_in_days', 'event_in_hours']] = [*train_df[predict_filter].apply(lambda row: get_timestamp_in_parts(row), axis=1)]\n",
    "    train_df['prior_minutes'] = train_df[predict_filter].apply(lambda row: get_prior_elasped_time_in_parts(row), axis=1)\n",
    "\n",
    "    # ##### 2. Find the intro rows.\n",
    "    # TODO: The intro section can also be calculated based on the bundle id\n",
    "    train_df['intro_section'] = train_df[predict_filter].apply(lambda row: ((train_df.timestamp == 0) or (row.task_container_id == 0)), axis=1)\n",
    "    train_df.head()\n",
    "\n",
    "    # ##### 3. The actions after the lectures should be tagged with something.\n",
    "    predicting_users_df = train_df[must_predict_users]\n",
    "    predicting_users_df[\"retention\"], predicting_users_df[\"previous_lecture_tag\"], predicting_users_df[\"previous_lecture_part\"],_predicingt_usesr_df[\"has_seen_lecture_before\"], predicting_users_df[\"has_seen_same_tag_as_lecture\"], predicting_users_df[\"has_seen_same_part_as_lecture\"],_predicingt_usesr_df[\"has_part_common_with_type_of\"], predicting_users_df[\"has_tag_common_with_type_of\"], predicting_users_df[\"has_tag_common_with_part_dict\"],_predicingt_usesr_df[\"has_type_of_common_with_part_dict\"] = zip(*predicting_users_df.apply(running_retention, axis=1))\n",
    "\n",
    "    # Make train df only question and not lectures\n",
    "    predicting_users_df = predicting_users_df[predicting_users_df['content_type_id'] == 0]\n",
    "\n",
    "    # ##### 4. Now lets find out the lag time\n",
    "    predicting_users_df[['event_time', 'shift_event_time', 'shift_elapsed_time', 'has_seen_question_explanation', 'event_lag_time']] = get_shift_values(predicting_users_df)\n",
    "    # ======================================\n",
    "    # train_df_elapsed_time_groupby = predicting_users_df[['content_id', 'shift_elapsed_time']].groupby(['content_id']).shift_elapsed_time\n",
    "    # individual_question_min_time_dict = train_df_elapsed_time_groupby.min().to_dict()\n",
    "    # individual_question_mean_time_dict = train_df_elapsed_time_groupby.mean().to_dict()\n",
    "\n",
    "    # average_question_timestamp_difference_dict = train_df[(train_df['shift_event_time'] < 3600000)][['content_id', 'shift_event_time']].groupby('content_id').shift_event_time.mean().to_dict()\n",
    "\n",
    "    # del train_df_elapsed_time_groupby\n",
    "    # ======================================\n",
    "    predicting_users_df['lag_time'] = predicting_users_df.apply(lambda row: get_prior_elapsed_time_difference(row.content_id, row.shift_elapsed_time), axis=1)\n",
    "    predicting_users_df['bundle_lag_time'] = predicting_users_df.apply(lambda row: get_bundle_lag_time(row), axis=1)\n",
    "\n",
    "    predicting_users_df['question_answered_late'] = predicting_users_df.apply(lambda row: has_elapsed_time_greater_than_average_time(row.content_id, row.shift_elapsed_time), axis=1)\n",
    "    predicting_users_df['event_time_greater_than_average'] = predicting_users_df.apply(lambda row: has_event_time_greater_than_average(row.content_id, row.shift_event_time), axis=1)\n",
    "\n",
    "    # ##### 5. lets find the average time took for each question\n",
    "    # The average time per question can be considered based on the every candidate or based on the current candidate previous question answering time.\n",
    "    # ======================================\n",
    "    # check average user answering time\n",
    "    user_average_time_to_elapsed_dict = predicting_users_df.groupby(\"user_id\").prior_question_elapsed_time.mean().to_dict()\n",
    "    # check average question answering time\n",
    "    average_question_prior_question_elapsed_time_dict = predicting_users_df.groupby([\"content_id\"]).prior_question_elapsed_time.mean().to_dict()\n",
    "    # ======================================\n",
    "    predicting_users_df['question_took_more_than_average_user_time'] = predicting_users_df.apply(lambda row: row.prior_question_elapsed_time > user_average_time_to_elapsed_dict[row.user_id], axis=1)\n",
    "    predicting_users_df['question_took_more_than_average_content_time'] = predicting_users_df.apply(lambda row: row.prior_question_elapsed_time > average_question_prior_question_elapsed_time_dict[row.user_id], axis=1)\n",
    "\n",
    "    predicting_users_df['has_more_than_average_bundle_time'] = predicting_users_df.apply(lambda row: row.shift_elapsed_time < bundle_time_relation_dict[row.content_id]['average_time'], axis=1)\n",
    "\n",
    "    # ##### 6. Find the toughest questions\n",
    "    predicting_users_df['question_has_above_average_correctness'] = predicting_users_df.apply(lambda row: is_question_above_average_answering(row), axis=1)\n",
    "    predicting_users_df['tag_has_above_average_correctness'] = predicting_users_df.apply(lambda row: is_all_tags_above_average_answering(row), axis=1)\n",
    "\n",
    "    # ##### 6. toughtest questions relates to the lag time and the elapsed time\n",
    "    lag_time_answered_correctly_mean_dict = predicting_users_df[check_answered_correctly][['content_id', 'lag_time']].groupby(['content_id']).lag_time.mean().to_dict()\n",
    "    predicting_users_df['has_above_average_lag_time_for_the_question'] = predicting_users_df.apply(lambda row: row.lag_time > lag_time_answered_correctly_mean_dict.get(row.content_id, 0), axis=1)\n",
    "\n",
    "    shift_elapsed_time_answered_correctly_mean_dict = predicting_users_df[check_answered_correctly][['content_id', 'shift_elapsed_time']].groupby(['content_id']).shift_elapsed_time.mean().to_dict()\n",
    "    train_df['has_above_average_shift_elpased_time_for_the_question'] = predicting_users_df.apply(lambda row: row.shift_elapsed_time > shift_elapsed_time_answered_correctly_mean_dict.get(row.content_id, 0), axis=1)\n",
    "\n",
    "    # ##### 7. The prior time mean for each user should be averaged to check if the user too more than usual\n",
    "    # TODO: Do the following by setting the time rather than setting the flag\n",
    "    # BUG: The comparison seems to be wrong \n",
    "    elapsed_time_mean_dict = predicting_users_df[check_answered_correctly][['shift_elapsed_time', 'user_id']].groupby('user_id').shift_elapsed_time.mean().to_dict()\n",
    "    predicting_users_df['has_above_user_average_time_to_answer'] = predicting_users_df[['user_id', 'shift_elapsed_time']].apply(lambda row: elapsed_time_mean_dict.get(row.user_id, 0) <= row.shift_elapsed_time, axis=1)\n",
    "\n",
    "    event_time_mean_dict = predicting_users_df[check_answered_correctly][['event_time', 'user_id']].groupby('user_id').event_time.mean().to_dict()\n",
    "    predicting_users_df['has_above_user_average_time_for_event'] = predicting_users_df[['user_id', 'event_time']].apply(lambda row: event_time_mean_dict.get(row.user_id, 0) <= row.event_time, axis=1)\n",
    "\n",
    "    shift_event_time_mean_dict = predicting_users_df[check_answered_correctly][['shift_event_time', 'user_id']].groupby('user_id').shift_event_time.mean().to_dict()\n",
    "    predicting_users_df['has_above_user_average_time_for_event'] = predicting_users_df[['user_id', 'shift_event_time']].apply(lambda row: shift_event_time_mean_dict.get(row.user_id, 0) <= row.shift_event_time, axis=1)\n",
    "\n",
    "    # ##### 10. The attempt feature\n",
    "    predicting_users_df[\"attempt_no\"] = 1\n",
    "    predicting_users_df.attempt_no = predicting_users_df.attempt_no.astype('int8')\n",
    "    predicting_users_df[\"attempt_no\"] = predicting_users_df[[\"user_id\", \"content_id\", \"attempt_no\"]].groupby([\"user_id\", \"content_id\"])[\"attempt_no\"].cumsum()\n",
    "    \n",
    "    # ##### 13. Check if the user clicked the correct answer by chance\n",
    "    predicting_users_df['luckily_answered_correctly'] = predicting_users_df.apply(lambda row: row.attempt_no == 0 and row.has_seen_question_explanation and row.answered_correctly, axis=1)\n",
    "    # ======================================\n",
    "\n",
    "    train_df[must_predict_users] = predicting_users_df\n",
    "    del predicting_users_df\n",
    "\n",
    "    # ======================================\n",
    "    # ##### 8. cum correctness of the answers made by the user\n",
    "    train_df[['user_cum_correctness', 'user_correct_cumsum', 'user_correct_cumcount']] = get_cum_sum_std_correctness(train_df, 'user_id', 'answered_correctly')\n",
    "\n",
    "    # ##### 9. cum of prior question had seen explanation \n",
    "    train_df[['seen_question_explanation_mean', 'seen_question_explanation_cumsum', 'seen_question_explanation_cumcount']] = get_cum_sum_std_correctness(train_df, 'user_id', 'has_seen_question_explanation')\n",
    "\n",
    "    # ##### 11. The aggregate of the answered correctly with the user_id, content_id\n",
    "    required_columns = ['user_id', 'answered_correctly']\n",
    "    train_df[['user_sum', 'user_std', 'user_correctness']] = get_sum_std_correctness(train_df, required_columns)\n",
    "\n",
    "    required_columns = ['content_id', 'answered_correctly']\n",
    "    train_df[['content_sum', 'content_std', 'content_correctness']] = get_sum_std_correctness(train_df, required_columns)\n",
    "\n",
    "    # ##### 12. get harmonic mean of the content\n",
    "    train_df['hmean_user_content_accuracy'] = 2 * (\n",
    "        (train_df['user_correctness'] * train_df['content_correctness']) /\n",
    "        (train_df['user_correctness'] + train_df['content_correctness'])\n",
    "    )\n",
    "\n",
    "    # ##### 13. The aggregate of the answered correctly with the bundle_id, part, part content type\n",
    "    required_columns = ['bundle_id', 'answered_correctly']\n",
    "    train_df[['bundle_sum', 'bundle_std', 'bundle_correctness']] = get_sum_std_correctness(train_df, required_columns)\n",
    "\n",
    "    required_columns = ['part', 'answered_correctly']\n",
    "    train_df[['part_sum', 'part_std', 'part_correctness']] = get_sum_std_correctness(train_df, required_columns)\n",
    "\n",
    "    required_columns = ['part_test_listening', 'answered_correctly']\n",
    "    train_df[['part_test_listening_sum', 'part_test_listening_std', 'part_test_listening_correctness']] = get_sum_std_correctness(train_df, required_columns)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4f505f879686e1f790cbcde16b7d0ad3222277933aef3abab0463f89f71cbbe9"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}